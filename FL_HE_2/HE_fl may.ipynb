{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import copy\n",
    "from clientClass import Client\n",
    "from dataFunction import *\n",
    "from HE_functions import *\n",
    "import tenseal as ts\n",
    "from cryptotree.preprocessing import Featurizer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "RANDOM_STATE = 123\n",
    "from collections import OrderedDict\n",
    "from base64 import b64encode, b64decode\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from time import time\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from scipy.special import expit\n",
    "from functools import reduce\n",
    "torch.random.manual_seed(11007303)\n",
    "random.seed(11007303)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # for (poly_mod, coeff_mod_bit_sizes) in [\n",
    "    #     (8192, [40, 21, 21, 21, 21, 21, 21, 40]),\n",
    "    #     (8192, [40, 20, 40]),\n",
    "    #     (8192, [20, 20, 20]),\n",
    "    #     (8192, [17, 17]),\n",
    "    #     (4096, [40, 20, 40]),\n",
    "    #     (4096, [30, 20, 30]),\n",
    "    #     (4096, [20, 20, 20]),\n",
    "    #     (4096, [19, 19, 19]),\n",
    "    #     (4096, [18, 18, 18]),\n",
    "    #     (4096, [18, 18]),\n",
    "    #     (4096, [17, 17]),\n",
    "    #     (2048, [20, 20]),\n",
    "    #     (2048, [18, 18]),\n",
    "    #     (2048, [16, 16]),\n",
    "# parameters\n",
    "poly_mod_degree = 4096\n",
    "coeff_mod_bit_sizes = [40, 20, 40]\n",
    "# create TenSEALContext\n",
    "ctx_eval = ts.context(ts.SCHEME_TYPE.CKKS, poly_mod_degree, -1, coeff_mod_bit_sizes)\n",
    "# scale of ciphertext to use\n",
    "ctx_eval.global_scale = 2 ** 20\n",
    "# this key is needed for doing dot-product operations\n",
    "ctx_eval.generate_galois_keys()\n",
    "# poly_mod = 2048 \n",
    "# coeff_mod_bit_sizes = [16, 16] \n",
    "# enc_type = ts.ENCRYPTION_TYPE.ASYMMETRIC #ts.ENCRYPTION_TYPE.SYMMETRIC\n",
    "\n",
    "# context = ts.context(\n",
    "#             scheme=ts.SCHEME_TYPE.CKKS,\n",
    "#             poly_modulus_degree=poly_mod,\n",
    "#             plain_modulus=786433,\n",
    "#             coeff_mod_bit_sizes=coeff_mod_bit_sizes,\n",
    "#             encryption_type=enc_type,\n",
    "#         )\n",
    "# context.generate_galois_keys()\n",
    "# # context.serialize()\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make Dataframes for every dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cleveland = \"FL_HE_2/processed.cleveland.data\"\n",
    "switzerland = \"FL_HE_2/processed.switzerland.data\"\n",
    "va = \"FL_HE_2/processed.va.data\"\n",
    "hungarian = \"FL_HE_2/reprocessed.hungarian.data\"\n",
    "cleveland_df, switzerland_df, va_df, hungarian_df = import_data(cleveland, switzerland, va, hungarian)\n",
    "df_dict ={\n",
    "    'Cleveland': cleveland_df\n",
    "    # ,\n",
    "    # 'Switzerland': switzerland_df,\n",
    "    # 'VA Long Beach': va_df,\n",
    "    # 'Hungary': hungarian_df        \n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LR(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, n_features):\n",
    "        super(LR, self).__init__()\n",
    "        self.lr = torch.nn.Linear(n_features, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = torch.sigmoid(self.lr(x))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_clients = 4\n",
    "clients = []\n",
    "n_features = 18\n",
    "glob_model = LR(n_features)\n",
    "\n",
    "# client_models = [copy.deepcopy(glob_model) for _ in range(n_clients)]\n",
    "# client_optims = [torch.optim.SGD(model.parameters(), lr=lr) for model in client_models]\n",
    "for i in list(df_dict.keys()):\n",
    "    df = df_dict.get(i)\n",
    "    df_dict[i] = new_df(df)\n",
    "    location_data = new_df(df)\n",
    "    y = location_data.HeartDisease\n",
    "    location_data = location_data.drop(columns=\"HeartDisease\")\n",
    "    cat_feat = ['ChestPainType', 'RestingECG', 'ST_Slope']\n",
    "    location_data = make_dummies(location_data, cat_feat)\n",
    "    numeric_feature_names = ['Age', 'MaxHR', 'RestingBP',  'Cholesterol', 'Oldpeak']\n",
    "    for j in numeric_feature_names:\n",
    "        if location_data[j].std() != 0: \n",
    "            location_data[j] = (location_data[j] - location_data[j].mean()) / location_data[j].std()\n",
    "    x = location_data\n",
    "    # keys = context.generate_galois_keys()\n",
    "    client_model = copy.deepcopy(glob_model)\n",
    "    lr = 2\n",
    "    lr_decay = 1\n",
    "    clients.append(Client(i, x, y, cat_feat, client_model, lr, torch.nn.BCELoss()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1592,  1.0000, -0.0960,  ...,  0.0000,  0.0000,  1.0000],\n",
       "        [-0.0486,  1.0000, -1.2324,  ...,  0.0000,  1.0000,  0.0000],\n",
       "        [ 0.1727,  1.0000, -0.3801,  ...,  0.0000,  1.0000,  0.0000],\n",
       "        ...,\n",
       "        [ 0.2833,  1.0000, -0.0960,  ...,  0.0000,  1.0000,  0.0000],\n",
       "        [ 0.8365,  0.0000,  0.3585,  ...,  0.0000,  1.0000,  0.0000],\n",
       "        [ 0.6153,  0.0000,  1.4949,  ...,  0.0000,  0.0000,  1.0000]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_X_set = [client.X_test for client in clients]\n",
    "validation_y_set = [client.X_test for client in clients]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 80\n",
    "\n",
    "def train(model, criterion, x, y, epochs=10):\n",
    "    optim = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "    epoch_loss = []\n",
    "    for e in range(1, epochs + 1):\n",
    "        optim.zero_grad()\n",
    "        out = model(x)\n",
    "        loss = criterion(out, y)\n",
    "        print(f\"Loss at epoch {e}: {loss.data}\")\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "    print(f\"Final Loss at epoch {e}: {loss.data}\")\n",
    "    print(loss.item())\n",
    "    epoch_loss.append(loss.item())\n",
    "    return model.state_dict(), loss.item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "size_train_sets = []\n",
    "\n",
    "for client in clients:\n",
    "    pipe = Featurizer(cat_feat)\n",
    "    y_train, y_valid, X_train_normalized, X_valid_normalized = split_prep_data(client, pipe)\n",
    "    size_train_sets.append(X_train_normalized.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_loss_LR(model, x, y, criterion):\n",
    "    out = model(x)\n",
    "    correct = torch.abs(y - out) < 0.5\n",
    "    loss = criterion(out, y)\n",
    "    return correct.float().mean(), loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncryptedLR:\n",
    "    \n",
    "    def __init__(self, torch_lr):\n",
    "        # TenSEAL processes lists and not torch tensors,\n",
    "        # so we take out the parameters from the PyTorch model\n",
    "        self.weight = torch_lr.lr.weight.data.tolist()[0]\n",
    "        self.bias = torch_lr.lr.bias.data.tolist()\n",
    "        \n",
    "    def forward(self, enc_x):\n",
    "        # We don't need to perform sigmoid as this model\n",
    "        # will only be used for evaluation, and the label\n",
    "        # can be deduced without applying sigmoid\n",
    "        enc_out = enc_x.dot(self.weight) + self.bias\n",
    "        return enc_out\n",
    "    \n",
    "    def __call__(self, *args, **kwargs):\n",
    "        return self.forward(*args, **kwargs)\n",
    "        \n",
    "    ################################################\n",
    "    ## You can use the functions below to perform ##\n",
    "    ## the evaluation with an encrypted model     ##\n",
    "    ################################################\n",
    "    \n",
    "    def encrypt(self, context):\n",
    "        self.weight = ts.ckks_vector(context, self.weight)\n",
    "        self.bias = ts.ckks_vector(context, self.bias)\n",
    "        \n",
    "    def decrypt(self, context):\n",
    "        self.weight = self.weight.decrypt()\n",
    "        self.bias = self.bias.decrypt()\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at epoch 1: 0.6921181678771973\n",
      "Loss at epoch 2: 0.6812593340873718\n",
      "Loss at epoch 3: 0.671548068523407\n",
      "Loss at epoch 3: 0.671548068523407\n",
      "Loss at epoch 1: 0.5236610174179077\n",
      "Loss at epoch 2: 0.46238890290260315\n",
      "Loss at epoch 3: 0.41394180059432983\n",
      "Loss at epoch 3: 0.41394180059432983\n",
      "Loss at epoch 1: 0.5577795505523682\n",
      "Loss at epoch 2: 0.5494472980499268\n",
      "Loss at epoch 3: 0.5426748394966125\n",
      "Loss at epoch 3: 0.5426748394966125\n",
      "Loss at epoch 1: 0.7966405749320984\n",
      "Loss at epoch 2: 0.7686443328857422\n",
      "Loss at epoch 3: 0.7452135682106018\n",
      "Loss at epoch 3: 0.7452135682106018\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "div(): argument 'input' (position 1) must be Tensor, not CKKSVector",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/romyho/Documents/Master_Econometrie/Thesis/Python/FL_HE_2/HE_fl copy.ipynb Cell 15'\u001b[0m in \u001b[0;36m<cell line: 12>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/romyho/Documents/Master_Econometrie/Thesis/Python/FL_HE_2/HE_fl%20copy.ipynb#ch0000009?line=46'>47</a>\u001b[0m lr \u001b[39m*\u001b[39m\u001b[39m=\u001b[39m lr_decay\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/romyho/Documents/Master_Econometrie/Thesis/Python/FL_HE_2/HE_fl%20copy.ipynb#ch0000009?line=48'>49</a>\u001b[0m \u001b[39mfor\u001b[39;00m k \u001b[39min\u001b[39;00m w_glob\u001b[39m.\u001b[39mkeys():\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/romyho/Documents/Master_Econometrie/Thesis/Python/FL_HE_2/HE_fl%20copy.ipynb#ch0000009?line=49'>50</a>\u001b[0m     w_glob[k] \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mdiv(w_glob[k], \u001b[39m4\u001b[39;49m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/romyho/Documents/Master_Econometrie/Thesis/Python/FL_HE_2/HE_fl%20copy.ipynb#ch0000009?line=51'>52</a>\u001b[0m \u001b[39mfor\u001b[39;00m k \u001b[39min\u001b[39;00m w_glob\u001b[39m.\u001b[39mkeys():\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/romyho/Documents/Master_Econometrie/Thesis/Python/FL_HE_2/HE_fl%20copy.ipynb#ch0000009?line=52'>53</a>\u001b[0m     \u001b[39mif\u001b[39;00m k \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mlr.weight\u001b[39m\u001b[39m'\u001b[39m:\n",
      "\u001b[0;31mTypeError\u001b[0m: div(): argument 'input' (position 1) must be Tensor, not CKKSVector"
     ]
    }
   ],
   "source": [
    "encrypted_weights_models = {}\n",
    "encrypted_bias_models = {}\n",
    "validation_X_set = []\n",
    "validation_y_set = []\n",
    "loss_train = []\n",
    "net_best = None\n",
    "best_loss = None\n",
    "best_acc = None\n",
    "best_epoch = None\n",
    "results = []\n",
    "\n",
    "for iter in range(EPOCHS):\n",
    "    w_glob = None\n",
    "    loss_locals = []\n",
    "    for client in clients:\n",
    "        pipe = Featurizer(cat_feat)\n",
    "        y_train, y_valid, X_train_normalized, X_valid_normalized = split_prep_data(client, pipe)\n",
    "        n_features = X_train_normalized.shape[1]\n",
    "\n",
    "\n",
    "        \n",
    "        if client.name == 'Cleveland':\n",
    "            validation_X_set = X_valid_normalized\n",
    "            validation_y_set = y_valid\n",
    "        else:\n",
    "            validation_X_set = np.concatenate((validation_X_set, X_valid_normalized), axis=0)\n",
    "            validation_y_set = np.concatenate((validation_y_set, y_valid), axis=0)\n",
    "        w_local, loss = train(model, optim, criterion, X_train_normalized, y_train)\n",
    "        \n",
    "        loss_locals.append(copy.deepcopy(loss))\n",
    "        for key in w_local.keys():\n",
    "            if key == 'lr.weight':\n",
    "                var_list = w_local.get(key)[0].tolist()\n",
    "                encrypted_values = ts.ckks_vector(ctx_eval, var_list)\n",
    "                w_local[key] = encrypted_values\n",
    "            else:\n",
    "                var_list = w_local.get(key).tolist()\n",
    "                encrypted_bias = ts.CKKSTensor(ctx_eval,var_list)\n",
    "                w_local[key] = encrypted_bias\n",
    "        \n",
    "        if w_glob is None:\n",
    "            w_glob = copy.deepcopy(w_local)\n",
    "        else:\n",
    "            for k in w_glob.keys():\n",
    "                w_glob[k] += w_local[k]\n",
    "\n",
    "    lr *= lr_decay\n",
    "\n",
    "    for k in w_glob.keys():\n",
    "        w_glob[k] = torch.div(w_glob[k], 4)\n",
    "\n",
    "        for k in w_glob.keys():\n",
    "        if k == 'lr.weight':\n",
    "            w_glob[k] = torch.Tensor(w_glob[k].decrypt()).unsqueeze(0)\n",
    "        else:\n",
    "             w_glob[k] = torch.Tensor(w_glob[k].decrypt().tolist())\n",
    "\n",
    "    net_glob.load_state_dict(w_glob)\n",
    "    loss_avg = sum(loss_locals) / len(loss_locals)\n",
    "    loss_train.append(loss_avg)\n",
    "    if (iter + 1) % 4 == 0:\n",
    "        net_glob.eval()\n",
    "        acc_test, loss_test =  accuracy_loss_LR(net_glob, validation_X_set_tensor, validation_y_set_tensor.reshape(validation_y_set_tensor.shape[:2]), criterion)\n",
    "\n",
    "        print('Round {:3d}, Average loss {:.3f}, Test loss {:.3f}, Test accuracy: {:.2f}'.format(\n",
    "            iter, loss_avg, loss_test, acc_test))\n",
    "\n",
    "\n",
    "        if best_acc is None or acc_test > best_acc:\n",
    "            net_best = copy.deepcopy(net_glob)\n",
    "            best_acc = acc_test\n",
    "            best_epoch = iter\n",
    "\n",
    "        # if (iter + 1) > args.start_saving:\n",
    "        #     model_save_path = os.path.join(base_dir, 'fed/model_{}.pt'.format(iter + 1))\n",
    "        #     torch.save(net_glob.state_dict(), model_save_path)\n",
    "\n",
    "        results.append(np.array([iter, loss_avg, loss_test, acc_test, best_acc]))\n",
    "        final_results = np.array(results)\n",
    "        final_results = pd.DataFrame(final_results, columns=['epoch', 'loss_avg', 'loss_test', 'acc_test', 'best_acc'])\n",
    "        # final_results.to_csv(results_save_path, index=False)\n",
    "\n",
    "    # if (iter + 1) % 50 == 0:\n",
    "        # best_save_path = os.path.join(base_dir, 'fed/best_{}.pt'.format(iter + 1))\n",
    "        # model_save_path = os.path.join(base_dir, 'fed/model_{}.pt'.format(iter + 1))\n",
    "        # torch.save(net_best.state_dict(), best_save_path)\n",
    "        # torch.save(net_glob.state_dict(), model_save_path)\n",
    "\n",
    "print('Best model, iter: {}, acc: {}'.format(best_epoch, best_acc))\n",
    "\n",
    "    # validation_y_set_tensor = torch.Tensor(validation_y_set).unsqueeze(1)\n",
    "    # validation_X_set_tensor = torch.tensor(validation_X_set).float()\n",
    "\n",
    "    # plain_accuracy = accuracy_LR(net_glob, validation_X_set_tensor, validation_y_set_tensor)\n",
    "    # print(f\"Accuracy on plain test_set: {plain_accuracy}\")\n",
    "    # weights = model.lr.weight.data.tolist()[0]\n",
    "    # bias = model.lr.bias.data.tolist()\n",
    "\n",
    "    # encrypted_weights = ts.ckks_vector(ctx_eval, weights)\n",
    "    # encrypted_weights_models[client.name+'_LR'] = encrypted_weights\n",
    "    # encrypted_bias = ts.ckks_vector(ctx_eval, bias)\n",
    "    # encrypted_bias_models[client.name+'_LR'] = encrypted_bias\n",
    "    # eelr = EncryptedLR(model)       \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_state_dict(state_dicts):\n",
    "    result = copy.deepcopy(state_dicts[0])\n",
    "    for key in result:\n",
    "        for state_dict in state_dicts[1:]:\n",
    "            result[key] += state_dict[key]\n",
    "        result[key] /= len(state_dicts)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('lr.weight',\n",
       "              tensor([[-0.0055, -0.0905,  0.0812, -0.1370,  0.0315, -0.0453, -0.2317, -0.1670,\n",
       "                       -0.1335, -0.0807,  0.2090, -0.2217, -0.2339,  0.2296,  0.1423, -0.2275,\n",
       "                        0.2180, -0.1723]])),\n",
       "             ('lr.bias', tensor([0.0768]))])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glob_model.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at epoch 1: 0.4600014388561249\n",
      "Loss at epoch 2: 0.45679816603660583\n",
      "Loss at epoch 3: 0.4540364146232605\n",
      "Loss at epoch 4: 0.45161861181259155\n",
      "Loss at epoch 5: 0.4494742751121521\n",
      "Loss at epoch 6: 0.4475511908531189\n",
      "Loss at epoch 7: 0.4458100199699402\n",
      "Loss at epoch 8: 0.4442203938961029\n",
      "Loss at epoch 9: 0.4427587389945984\n",
      "Loss at epoch 10: 0.4414062201976776\n",
      "Final Loss at epoch 10: 0.4414062201976776\n",
      "0.4414062201976776\n",
      "Round   0, Average loss 0.441, Test loss 0.412, Test accuracy: 0.80\n",
      "Loss at epoch 1: 0.44014784693717957\n",
      "Loss at epoch 2: 0.4389713406562805\n",
      "Loss at epoch 3: 0.43786653876304626\n",
      "Loss at epoch 4: 0.436825156211853\n",
      "Loss at epoch 5: 0.43584007024765015\n",
      "Loss at epoch 6: 0.43490543961524963\n",
      "Loss at epoch 7: 0.4340161681175232\n",
      "Loss at epoch 8: 0.43316781520843506\n",
      "Loss at epoch 9: 0.4323567748069763\n",
      "Loss at epoch 10: 0.43157970905303955\n",
      "Final Loss at epoch 10: 0.43157970905303955\n",
      "0.43157970905303955\n",
      "Round   1, Average loss 0.432, Test loss 0.403, Test accuracy: 0.80\n",
      "Loss at epoch 1: 0.43083372712135315\n",
      "Loss at epoch 2: 0.43011629581451416\n",
      "Loss at epoch 3: 0.42942532896995544\n",
      "Loss at epoch 4: 0.4287587106227875\n",
      "Loss at epoch 5: 0.42811471223831177\n",
      "Loss at epoch 6: 0.4274918735027313\n",
      "Loss at epoch 7: 0.4268887937068939\n",
      "Loss at epoch 8: 0.4263039827346802\n",
      "Loss at epoch 9: 0.42573660612106323\n",
      "Loss at epoch 10: 0.42518553137779236\n",
      "Final Loss at epoch 10: 0.42518553137779236\n",
      "0.42518553137779236\n",
      "Round   2, Average loss 0.425, Test loss 0.400, Test accuracy: 0.77\n",
      "Loss at epoch 1: 0.4246496856212616\n",
      "Loss at epoch 2: 0.42412832379341125\n",
      "Loss at epoch 3: 0.42362070083618164\n",
      "Loss at epoch 4: 0.42312613129615784\n",
      "Loss at epoch 5: 0.42264384031295776\n",
      "Loss at epoch 6: 0.42217329144477844\n",
      "Loss at epoch 7: 0.42171403765678406\n",
      "Loss at epoch 8: 0.4212653338909149\n",
      "Loss at epoch 9: 0.42082691192626953\n",
      "Loss at epoch 10: 0.4203982353210449\n",
      "Final Loss at epoch 10: 0.4203982353210449\n",
      "0.4203982353210449\n",
      "Round   3, Average loss 0.420, Test loss 0.398, Test accuracy: 0.79\n",
      "Loss at epoch 1: 0.4199790060520172\n",
      "Loss at epoch 2: 0.4195686876773834\n",
      "Loss at epoch 3: 0.41916701197624207\n",
      "Loss at epoch 4: 0.4187736511230469\n",
      "Loss at epoch 5: 0.4183882772922516\n",
      "Loss at epoch 6: 0.4180106222629547\n",
      "Loss at epoch 7: 0.41764035820961\n",
      "Loss at epoch 8: 0.41727733612060547\n",
      "Loss at epoch 9: 0.4169212579727173\n",
      "Loss at epoch 10: 0.41657179594039917\n",
      "Final Loss at epoch 10: 0.41657179594039917\n",
      "0.41657179594039917\n",
      "Round   4, Average loss 0.417, Test loss 0.397, Test accuracy: 0.77\n",
      "Loss at epoch 1: 0.41622889041900635\n",
      "Loss at epoch 2: 0.41589224338531494\n",
      "Loss at epoch 3: 0.41556164622306824\n",
      "Loss at epoch 4: 0.4152370095252991\n",
      "Loss at epoch 5: 0.41491806507110596\n",
      "Loss at epoch 6: 0.41460469365119934\n",
      "Loss at epoch 7: 0.4142967164516449\n",
      "Loss at epoch 8: 0.4139940142631531\n",
      "Loss at epoch 9: 0.41369640827178955\n",
      "Loss at epoch 10: 0.41340377926826477\n",
      "Final Loss at epoch 10: 0.41340377926826477\n",
      "0.41340377926826477\n",
      "Round   5, Average loss 0.413, Test loss 0.397, Test accuracy: 0.77\n",
      "Loss at epoch 1: 0.4131159782409668\n",
      "Loss at epoch 2: 0.4128328859806061\n",
      "Loss at epoch 3: 0.41255441308021545\n",
      "Loss at epoch 4: 0.4122803807258606\n",
      "Loss at epoch 5: 0.41201069951057434\n",
      "Loss at epoch 6: 0.41174525022506714\n",
      "Loss at epoch 7: 0.4114840030670166\n",
      "Loss at epoch 8: 0.4112268388271332\n",
      "Loss at epoch 9: 0.41097357869148254\n",
      "Loss at epoch 10: 0.41072413325309753\n",
      "Final Loss at epoch 10: 0.41072413325309753\n",
      "0.41072413325309753\n",
      "Round   6, Average loss 0.411, Test loss 0.397, Test accuracy: 0.79\n",
      "Loss at epoch 1: 0.41047853231430054\n",
      "Loss at epoch 2: 0.41023656725883484\n",
      "Loss at epoch 3: 0.4099982678890228\n",
      "Loss at epoch 4: 0.40976348519325256\n",
      "Loss at epoch 5: 0.4095320701599121\n",
      "Loss at epoch 6: 0.40930405259132385\n",
      "Loss at epoch 7: 0.40907934308052063\n",
      "Loss at epoch 8: 0.4088578224182129\n",
      "Loss at epoch 9: 0.408639520406723\n",
      "Loss at epoch 10: 0.4084242582321167\n",
      "Final Loss at epoch 10: 0.4084242582321167\n",
      "0.4084242582321167\n",
      "Round   7, Average loss 0.408, Test loss 0.398, Test accuracy: 0.79\n",
      "Loss at epoch 1: 0.4082120656967163\n",
      "Loss at epoch 2: 0.4080027937889099\n",
      "Loss at epoch 3: 0.4077964723110199\n",
      "Loss at epoch 4: 0.4075929820537567\n",
      "Loss at epoch 5: 0.4073922634124756\n",
      "Loss at epoch 6: 0.40719425678253174\n",
      "Loss at epoch 7: 0.4069989323616028\n",
      "Loss at epoch 8: 0.4068062901496887\n",
      "Loss at epoch 9: 0.4066162109375\n",
      "Loss at epoch 10: 0.40642857551574707\n",
      "Final Loss at epoch 10: 0.40642857551574707\n",
      "0.40642857551574707\n",
      "Round   8, Average loss 0.406, Test loss 0.398, Test accuracy: 0.79\n",
      "Loss at epoch 1: 0.4062435030937195\n",
      "Loss at epoch 2: 0.4060608148574829\n",
      "Loss at epoch 3: 0.40588051080703735\n",
      "Loss at epoch 4: 0.40570253133773804\n",
      "Loss at epoch 5: 0.40552690625190735\n",
      "Loss at epoch 6: 0.40535348653793335\n",
      "Loss at epoch 7: 0.4051823019981384\n",
      "Loss at epoch 8: 0.4050133228302002\n",
      "Loss at epoch 9: 0.4048463702201843\n",
      "Loss at epoch 10: 0.40468159317970276\n",
      "Final Loss at epoch 10: 0.40468159317970276\n",
      "0.40468159317970276\n",
      "Round   9, Average loss 0.405, Test loss 0.399, Test accuracy: 0.79\n",
      "Loss at epoch 1: 0.40451884269714355\n",
      "Loss at epoch 2: 0.4043580889701843\n",
      "Loss at epoch 3: 0.40419939160346985\n",
      "Loss at epoch 4: 0.4040426015853882\n",
      "Loss at epoch 5: 0.40388771891593933\n",
      "Loss at epoch 6: 0.4037347733974457\n",
      "Loss at epoch 7: 0.4035836160182953\n",
      "Loss at epoch 8: 0.40343430638313293\n",
      "Loss at epoch 9: 0.40328675508499146\n",
      "Loss at epoch 10: 0.4031410217285156\n",
      "Final Loss at epoch 10: 0.4031410217285156\n",
      "0.4031410217285156\n",
      "Round  10, Average loss 0.403, Test loss 0.400, Test accuracy: 0.80\n",
      "Loss at epoch 1: 0.4029969274997711\n",
      "Loss at epoch 2: 0.4028545320034027\n",
      "Loss at epoch 3: 0.4027138650417328\n",
      "Loss at epoch 4: 0.4025748372077942\n",
      "Loss at epoch 5: 0.4024374186992645\n",
      "Loss at epoch 6: 0.4023016095161438\n",
      "Loss at epoch 7: 0.40216729044914246\n",
      "Loss at epoch 8: 0.40203458070755005\n",
      "Loss at epoch 9: 0.40190333127975464\n",
      "Loss at epoch 10: 0.4017735719680786\n",
      "Final Loss at epoch 10: 0.4017735719680786\n",
      "0.4017735719680786\n",
      "Round  11, Average loss 0.402, Test loss 0.401, Test accuracy: 0.79\n",
      "Loss at epoch 1: 0.401645302772522\n",
      "Loss at epoch 2: 0.40151846408843994\n",
      "Loss at epoch 3: 0.40139302611351013\n",
      "Loss at epoch 4: 0.4012690484523773\n",
      "Loss at epoch 5: 0.4011463522911072\n",
      "Loss at epoch 6: 0.40102508664131165\n",
      "Loss at epoch 7: 0.4009051024913788\n",
      "Loss at epoch 8: 0.4007863998413086\n",
      "Loss at epoch 9: 0.4006690979003906\n",
      "Loss at epoch 10: 0.40055298805236816\n",
      "Final Loss at epoch 10: 0.40055298805236816\n",
      "0.40055298805236816\n",
      "Round  12, Average loss 0.401, Test loss 0.402, Test accuracy: 0.79\n",
      "Loss at epoch 1: 0.4004381000995636\n",
      "Loss at epoch 2: 0.4003245234489441\n",
      "Loss at epoch 3: 0.4002121090888977\n",
      "Loss at epoch 4: 0.4001008868217468\n",
      "Loss at epoch 5: 0.39999085664749146\n",
      "Loss at epoch 6: 0.399882048368454\n",
      "Loss at epoch 7: 0.3997742831707001\n",
      "Loss at epoch 8: 0.3996676802635193\n",
      "Loss at epoch 9: 0.39956220984458923\n",
      "Loss at epoch 10: 0.39945781230926514\n",
      "Final Loss at epoch 10: 0.39945781230926514\n",
      "0.39945781230926514\n",
      "Round  13, Average loss 0.399, Test loss 0.403, Test accuracy: 0.79\n",
      "Loss at epoch 1: 0.399354487657547\n",
      "Loss at epoch 2: 0.39925214648246765\n",
      "Loss at epoch 3: 0.39915093779563904\n",
      "Loss at epoch 4: 0.399050772190094\n",
      "Loss at epoch 5: 0.39895159006118774\n",
      "Loss at epoch 6: 0.3988533914089203\n",
      "Loss at epoch 7: 0.3987562358379364\n",
      "Loss at epoch 8: 0.39865997433662415\n",
      "Loss at epoch 9: 0.3985646963119507\n",
      "Loss at epoch 10: 0.398470401763916\n",
      "Final Loss at epoch 10: 0.398470401763916\n",
      "0.398470401763916\n",
      "Round  14, Average loss 0.398, Test loss 0.404, Test accuracy: 0.79\n",
      "Loss at epoch 1: 0.39837703108787537\n",
      "Loss at epoch 2: 0.39828455448150635\n",
      "Loss at epoch 3: 0.39819303154945374\n",
      "Loss at epoch 4: 0.398102343082428\n",
      "Loss at epoch 5: 0.39801254868507385\n",
      "Loss at epoch 6: 0.39792364835739136\n",
      "Loss at epoch 7: 0.3978356122970581\n",
      "Loss at epoch 8: 0.3977483808994293\n",
      "Loss at epoch 9: 0.3976620137691498\n",
      "Loss at epoch 10: 0.3975764513015747\n",
      "Final Loss at epoch 10: 0.3975764513015747\n",
      "0.3975764513015747\n",
      "Round  15, Average loss 0.398, Test loss 0.405, Test accuracy: 0.79\n",
      "Loss at epoch 1: 0.3974916934967041\n",
      "Loss at epoch 2: 0.39740774035453796\n",
      "Loss at epoch 3: 0.3973245918750763\n",
      "Loss at epoch 4: 0.3972422480583191\n",
      "Loss at epoch 5: 0.3971606492996216\n",
      "Loss at epoch 6: 0.39707982540130615\n",
      "Loss at epoch 7: 0.396999716758728\n",
      "Loss at epoch 8: 0.3969203233718872\n",
      "Loss at epoch 9: 0.39684170484542847\n",
      "Loss at epoch 10: 0.3967638313770294\n",
      "Final Loss at epoch 10: 0.3967638313770294\n",
      "0.3967638313770294\n",
      "Round  16, Average loss 0.397, Test loss 0.407, Test accuracy: 0.79\n",
      "Loss at epoch 1: 0.3966866433620453\n",
      "Loss at epoch 2: 0.3966101408004761\n",
      "Loss at epoch 3: 0.3965343236923218\n",
      "Loss at epoch 4: 0.3964592218399048\n",
      "Loss at epoch 5: 0.3963847756385803\n",
      "Loss at epoch 6: 0.3963109850883484\n",
      "Loss at epoch 7: 0.396237850189209\n",
      "Loss at epoch 8: 0.3961654007434845\n",
      "Loss at epoch 9: 0.39609357714653015\n",
      "Loss at epoch 10: 0.39602234959602356\n",
      "Final Loss at epoch 10: 0.39602234959602356\n",
      "0.39602234959602356\n",
      "Round  17, Average loss 0.396, Test loss 0.408, Test accuracy: 0.79\n",
      "Loss at epoch 1: 0.3959518074989319\n",
      "Loss at epoch 2: 0.3958818316459656\n",
      "Loss at epoch 3: 0.3958125114440918\n",
      "Loss at epoch 4: 0.39574378728866577\n",
      "Loss at epoch 5: 0.3956756293773651\n",
      "Loss at epoch 6: 0.3956080675125122\n",
      "Loss at epoch 7: 0.39554110169410706\n",
      "Loss at epoch 8: 0.3954746723175049\n",
      "Loss at epoch 9: 0.39540883898735046\n",
      "Loss at epoch 10: 0.3953435719013214\n",
      "Final Loss at epoch 10: 0.3953435719013214\n",
      "0.3953435719013214\n",
      "Round  18, Average loss 0.395, Test loss 0.409, Test accuracy: 0.79\n",
      "Loss at epoch 1: 0.3952788710594177\n",
      "Loss at epoch 2: 0.39521467685699463\n",
      "Loss at epoch 3: 0.3951510190963745\n",
      "Loss at epoch 4: 0.39508795738220215\n",
      "Loss at epoch 5: 0.3950254023075104\n",
      "Loss at epoch 6: 0.3949633538722992\n",
      "Loss at epoch 7: 0.3949017822742462\n",
      "Loss at epoch 8: 0.3948407769203186\n",
      "Loss at epoch 9: 0.3947802782058716\n",
      "Loss at epoch 10: 0.39472025632858276\n",
      "Final Loss at epoch 10: 0.39472025632858276\n",
      "0.39472025632858276\n",
      "Round  19, Average loss 0.395, Test loss 0.410, Test accuracy: 0.79\n",
      "Loss at epoch 1: 0.39466071128845215\n",
      "Loss at epoch 2: 0.39460164308547974\n",
      "Loss at epoch 3: 0.3945431113243103\n",
      "Loss at epoch 4: 0.3944849669933319\n",
      "Loss at epoch 5: 0.3944273293018341\n",
      "Loss at epoch 6: 0.3943701982498169\n",
      "Loss at epoch 7: 0.3943135142326355\n",
      "Loss at epoch 8: 0.3942572772502899\n",
      "Loss at epoch 9: 0.39420145750045776\n",
      "Loss at epoch 10: 0.3941460847854614\n",
      "Final Loss at epoch 10: 0.3941460847854614\n",
      "0.3941460847854614\n",
      "Round  20, Average loss 0.394, Test loss 0.411, Test accuracy: 0.79\n",
      "Loss at epoch 1: 0.3940911591053009\n",
      "Loss at epoch 2: 0.3940366804599762\n",
      "Loss at epoch 3: 0.3939826488494873\n",
      "Loss at epoch 4: 0.39392903447151184\n",
      "Loss at epoch 5: 0.3938758075237274\n",
      "Loss at epoch 6: 0.3938229978084564\n",
      "Loss at epoch 7: 0.39377060532569885\n",
      "Loss at epoch 8: 0.3937186598777771\n",
      "Loss at epoch 9: 0.3936671018600464\n",
      "Loss at epoch 10: 0.3936159014701843\n",
      "Final Loss at epoch 10: 0.3936159014701843\n",
      "0.3936159014701843\n",
      "Round  21, Average loss 0.394, Test loss 0.412, Test accuracy: 0.79\n",
      "Loss at epoch 1: 0.3935651183128357\n",
      "Loss at epoch 2: 0.3935147523880005\n",
      "Loss at epoch 3: 0.39346471428871155\n",
      "Loss at epoch 4: 0.39341509342193604\n",
      "Loss at epoch 5: 0.3933658003807068\n",
      "Loss at epoch 6: 0.39331695437431335\n",
      "Loss at epoch 7: 0.3932684659957886\n",
      "Loss at epoch 8: 0.39322027564048767\n",
      "Loss at epoch 9: 0.3931725323200226\n",
      "Loss at epoch 10: 0.39312508702278137\n",
      "Final Loss at epoch 10: 0.39312508702278137\n",
      "0.39312508702278137\n",
      "Round  22, Average loss 0.393, Test loss 0.414, Test accuracy: 0.79\n",
      "Loss at epoch 1: 0.3930779695510864\n",
      "Loss at epoch 2: 0.3930312395095825\n",
      "Loss at epoch 3: 0.3929848372936249\n",
      "Loss at epoch 4: 0.39293885231018066\n",
      "Loss at epoch 5: 0.3928931653499603\n",
      "Loss at epoch 6: 0.39284777641296387\n",
      "Loss at epoch 7: 0.3928026854991913\n",
      "Loss at epoch 8: 0.39275801181793213\n",
      "Loss at epoch 9: 0.39271366596221924\n",
      "Loss at epoch 10: 0.3926696181297302\n",
      "Final Loss at epoch 10: 0.3926696181297302\n",
      "0.3926696181297302\n",
      "Round  23, Average loss 0.393, Test loss 0.415, Test accuracy: 0.79\n",
      "Loss at epoch 1: 0.3926258683204651\n",
      "Loss at epoch 2: 0.39258241653442383\n",
      "Loss at epoch 3: 0.3925393223762512\n",
      "Loss at epoch 4: 0.3924965262413025\n",
      "Loss at epoch 5: 0.39245402812957764\n",
      "Loss at epoch 6: 0.39241188764572144\n",
      "Loss at epoch 7: 0.39236995577812195\n",
      "Loss at epoch 8: 0.3923283517360687\n",
      "Loss at epoch 9: 0.39228710532188416\n",
      "Loss at epoch 10: 0.3922460675239563\n",
      "Final Loss at epoch 10: 0.3922460675239563\n",
      "0.3922460675239563\n",
      "Round  24, Average loss 0.392, Test loss 0.416, Test accuracy: 0.79\n",
      "Loss at epoch 1: 0.3922053277492523\n",
      "Loss at epoch 2: 0.3921649158000946\n",
      "Loss at epoch 3: 0.3921247720718384\n",
      "Loss at epoch 4: 0.39208489656448364\n",
      "Loss at epoch 5: 0.3920453190803528\n",
      "Loss at epoch 6: 0.392005980014801\n",
      "Loss at epoch 7: 0.3919669985771179\n",
      "Loss at epoch 8: 0.39192819595336914\n",
      "Loss at epoch 9: 0.3918897211551666\n",
      "Loss at epoch 10: 0.3918514847755432\n",
      "Final Loss at epoch 10: 0.3918514847755432\n",
      "0.3918514847755432\n",
      "Round  25, Average loss 0.392, Test loss 0.417, Test accuracy: 0.79\n",
      "Loss at epoch 1: 0.3918135166168213\n",
      "Loss at epoch 2: 0.39177578687667847\n",
      "Loss at epoch 3: 0.39173829555511475\n",
      "Loss at epoch 4: 0.3917011320590973\n",
      "Loss at epoch 5: 0.39166417717933655\n",
      "Loss at epoch 6: 0.3916274905204773\n",
      "Loss at epoch 7: 0.39159104228019714\n",
      "Loss at epoch 8: 0.3915548324584961\n",
      "Loss at epoch 9: 0.39151886105537415\n",
      "Loss at epoch 10: 0.3914831280708313\n",
      "Final Loss at epoch 10: 0.3914831280708313\n",
      "0.3914831280708313\n",
      "Round  26, Average loss 0.391, Test loss 0.418, Test accuracy: 0.79\n",
      "Loss at epoch 1: 0.39144766330718994\n",
      "Loss at epoch 2: 0.3914124071598053\n",
      "Loss at epoch 3: 0.39137744903564453\n",
      "Loss at epoch 4: 0.3913426399230957\n",
      "Loss at epoch 5: 0.391308069229126\n",
      "Loss at epoch 6: 0.3912737965583801\n",
      "Loss at epoch 7: 0.3912397027015686\n",
      "Loss at epoch 8: 0.3912058472633362\n",
      "Loss at epoch 9: 0.3911722004413605\n",
      "Loss at epoch 10: 0.3911387622356415\n",
      "Final Loss at epoch 10: 0.3911387622356415\n",
      "0.3911387622356415\n",
      "Round  27, Average loss 0.391, Test loss 0.420, Test accuracy: 0.79\n",
      "Loss at epoch 1: 0.3911055624485016\n",
      "Loss at epoch 2: 0.39107251167297363\n",
      "Loss at epoch 3: 0.39103981852531433\n",
      "Loss at epoch 4: 0.3910072445869446\n",
      "Loss at epoch 5: 0.39097487926483154\n",
      "Loss at epoch 6: 0.3909427523612976\n",
      "Loss at epoch 7: 0.390910804271698\n",
      "Loss at epoch 8: 0.3908790946006775\n",
      "Loss at epoch 9: 0.3908475339412689\n",
      "Loss at epoch 10: 0.39081624150276184\n",
      "Final Loss at epoch 10: 0.39081624150276184\n",
      "0.39081624150276184\n",
      "Round  28, Average loss 0.391, Test loss 0.421, Test accuracy: 0.79\n",
      "Loss at epoch 1: 0.3907851576805115\n",
      "Loss at epoch 2: 0.39075419306755066\n",
      "Loss at epoch 3: 0.39072349667549133\n",
      "Loss at epoch 4: 0.39069297909736633\n",
      "Loss at epoch 5: 0.3906625807285309\n",
      "Loss at epoch 6: 0.3906325101852417\n",
      "Loss at epoch 7: 0.3906025290489197\n",
      "Loss at epoch 8: 0.39057278633117676\n",
      "Loss at epoch 9: 0.39054322242736816\n",
      "Loss at epoch 10: 0.3905138075351715\n",
      "Final Loss at epoch 10: 0.3905138075351715\n",
      "0.3905138075351715\n",
      "Round  29, Average loss 0.391, Test loss 0.422, Test accuracy: 0.79\n",
      "Loss at epoch 1: 0.39048460125923157\n",
      "Loss at epoch 2: 0.39045554399490356\n",
      "Loss at epoch 3: 0.39042672514915466\n",
      "Loss at epoch 4: 0.3903980553150177\n",
      "Loss at epoch 5: 0.39036962389945984\n",
      "Loss at epoch 6: 0.39034125208854675\n",
      "Loss at epoch 7: 0.39031314849853516\n",
      "Loss at epoch 8: 0.3902852535247803\n",
      "Loss at epoch 9: 0.3902573883533478\n",
      "Loss at epoch 10: 0.3902297914028168\n",
      "Final Loss at epoch 10: 0.3902297914028168\n",
      "0.3902297914028168\n",
      "Round  30, Average loss 0.390, Test loss 0.423, Test accuracy: 0.79\n",
      "Loss at epoch 1: 0.3902023434638977\n",
      "Loss at epoch 2: 0.3901750147342682\n",
      "Loss at epoch 3: 0.39014795422554016\n",
      "Loss at epoch 4: 0.3901210129261017\n",
      "Loss at epoch 5: 0.39009422063827515\n",
      "Loss at epoch 6: 0.39006757736206055\n",
      "Loss at epoch 7: 0.39004117250442505\n",
      "Loss at epoch 8: 0.3900148570537567\n",
      "Loss at epoch 9: 0.3899887204170227\n",
      "Loss at epoch 10: 0.38996270298957825\n",
      "Final Loss at epoch 10: 0.38996270298957825\n",
      "0.38996270298957825\n",
      "Round  31, Average loss 0.390, Test loss 0.424, Test accuracy: 0.79\n",
      "Loss at epoch 1: 0.3899368941783905\n",
      "Loss at epoch 2: 0.3899112343788147\n",
      "Loss at epoch 3: 0.38988569378852844\n",
      "Loss at epoch 4: 0.3898603320121765\n",
      "Loss at epoch 5: 0.3898351490497589\n",
      "Loss at epoch 6: 0.38981008529663086\n",
      "Loss at epoch 7: 0.38978514075279236\n",
      "Loss at epoch 8: 0.3897603750228882\n",
      "Loss at epoch 9: 0.38973578810691833\n",
      "Loss at epoch 10: 0.38971132040023804\n",
      "Final Loss at epoch 10: 0.38971132040023804\n",
      "0.38971132040023804\n",
      "Round  32, Average loss 0.390, Test loss 0.425, Test accuracy: 0.79\n",
      "Loss at epoch 1: 0.3896870017051697\n",
      "Loss at epoch 2: 0.38966280221939087\n",
      "Loss at epoch 3: 0.3896387815475464\n",
      "Loss at epoch 4: 0.38961485028266907\n",
      "Loss at epoch 5: 0.38959112763404846\n",
      "Loss at epoch 6: 0.389567494392395\n",
      "Loss at epoch 7: 0.3895440101623535\n",
      "Loss at epoch 8: 0.38952070474624634\n",
      "Loss at epoch 9: 0.38949745893478394\n",
      "Loss at epoch 10: 0.38947439193725586\n",
      "Final Loss at epoch 10: 0.38947439193725586\n",
      "0.38947439193725586\n",
      "Round  33, Average loss 0.389, Test loss 0.426, Test accuracy: 0.79\n",
      "Loss at epoch 1: 0.3894514739513397\n",
      "Loss at epoch 2: 0.38942864537239075\n",
      "Loss at epoch 3: 0.3894059956073761\n",
      "Loss at epoch 4: 0.389383465051651\n",
      "Loss at epoch 5: 0.38936102390289307\n",
      "Loss at epoch 6: 0.38933876156806946\n",
      "Loss at epoch 7: 0.389316588640213\n",
      "Loss at epoch 8: 0.3892945945262909\n",
      "Loss at epoch 9: 0.38927266001701355\n",
      "Loss at epoch 10: 0.38925087451934814\n",
      "Final Loss at epoch 10: 0.38925087451934814\n",
      "0.38925087451934814\n",
      "Round  34, Average loss 0.389, Test loss 0.427, Test accuracy: 0.77\n",
      "Loss at epoch 1: 0.3892292380332947\n",
      "Loss at epoch 2: 0.38920772075653076\n",
      "Loss at epoch 3: 0.389186292886734\n",
      "Loss at epoch 4: 0.3891650438308716\n",
      "Loss at epoch 5: 0.3891438841819763\n",
      "Loss at epoch 6: 0.3891228437423706\n",
      "Loss at epoch 7: 0.38910192251205444\n",
      "Loss at epoch 8: 0.38908112049102783\n",
      "Loss at epoch 9: 0.38906043767929077\n",
      "Loss at epoch 10: 0.3890398442745209\n",
      "Final Loss at epoch 10: 0.3890398442745209\n",
      "0.3890398442745209\n",
      "Round  35, Average loss 0.389, Test loss 0.429, Test accuracy: 0.77\n",
      "Loss at epoch 1: 0.3890193998813629\n",
      "Loss at epoch 2: 0.3889990746974945\n",
      "Loss at epoch 3: 0.38897883892059326\n",
      "Loss at epoch 4: 0.38895872235298157\n",
      "Loss at epoch 5: 0.3889387547969818\n",
      "Loss at epoch 6: 0.38891884684562683\n",
      "Loss at epoch 7: 0.3888990581035614\n",
      "Loss at epoch 8: 0.3888794779777527\n",
      "Loss at epoch 9: 0.3888598680496216\n",
      "Loss at epoch 10: 0.3888404071331024\n",
      "Final Loss at epoch 10: 0.3888404071331024\n",
      "0.3888404071331024\n",
      "Round  36, Average loss 0.389, Test loss 0.430, Test accuracy: 0.77\n",
      "Loss at epoch 1: 0.3888210952281952\n",
      "Loss at epoch 2: 0.38880181312561035\n",
      "Loss at epoch 3: 0.38878270983695984\n",
      "Loss at epoch 4: 0.3887636661529541\n",
      "Loss at epoch 5: 0.3887447714805603\n",
      "Loss at epoch 6: 0.38872596621513367\n",
      "Loss at epoch 7: 0.3887072503566742\n",
      "Loss at epoch 8: 0.38868868350982666\n",
      "Loss at epoch 9: 0.3886701464653015\n",
      "Loss at epoch 10: 0.3886517584323883\n",
      "Final Loss at epoch 10: 0.3886517584323883\n",
      "0.3886517584323883\n",
      "Round  37, Average loss 0.389, Test loss 0.431, Test accuracy: 0.77\n",
      "Loss at epoch 1: 0.3886334300041199\n",
      "Loss at epoch 2: 0.3886152505874634\n",
      "Loss at epoch 3: 0.38859716057777405\n",
      "Loss at epoch 4: 0.38857918977737427\n",
      "Loss at epoch 5: 0.3885612487792969\n",
      "Loss at epoch 6: 0.3885434567928314\n",
      "Loss at epoch 7: 0.38852572441101074\n",
      "Loss at epoch 8: 0.388508141040802\n",
      "Loss at epoch 9: 0.38849061727523804\n",
      "Loss at epoch 10: 0.38847315311431885\n",
      "Final Loss at epoch 10: 0.38847315311431885\n",
      "0.38847315311431885\n",
      "Round  38, Average loss 0.388, Test loss 0.432, Test accuracy: 0.77\n",
      "Loss at epoch 1: 0.388455867767334\n",
      "Loss at epoch 2: 0.3884385824203491\n",
      "Loss at epoch 3: 0.38842150568962097\n",
      "Loss at epoch 4: 0.38840439915657043\n",
      "Loss at epoch 5: 0.3883874714374542\n",
      "Loss at epoch 6: 0.3883706033229828\n",
      "Loss at epoch 7: 0.3883538246154785\n",
      "Loss at epoch 8: 0.3883371353149414\n",
      "Loss at epoch 9: 0.38832053542137146\n",
      "Loss at epoch 10: 0.3883040249347687\n",
      "Final Loss at epoch 10: 0.3883040249347687\n",
      "0.3883040249347687\n",
      "Round  39, Average loss 0.388, Test loss 0.433, Test accuracy: 0.77\n",
      "Loss at epoch 1: 0.38828757405281067\n",
      "Loss at epoch 2: 0.3882712423801422\n",
      "Loss at epoch 3: 0.3882550299167633\n",
      "Loss at epoch 4: 0.3882388472557068\n",
      "Loss at epoch 5: 0.38822275400161743\n",
      "Loss at epoch 6: 0.3882067799568176\n",
      "Loss at epoch 7: 0.3881908655166626\n",
      "Loss at epoch 8: 0.38817501068115234\n",
      "Loss at epoch 9: 0.38815930485725403\n",
      "Loss at epoch 10: 0.3881436586380005\n",
      "Final Loss at epoch 10: 0.3881436586380005\n",
      "0.3881436586380005\n",
      "Round  40, Average loss 0.388, Test loss 0.434, Test accuracy: 0.77\n",
      "Loss at epoch 1: 0.3881281018257141\n",
      "Loss at epoch 2: 0.38811254501342773\n",
      "Loss at epoch 3: 0.3880971670150757\n",
      "Loss at epoch 4: 0.388081818819046\n",
      "Loss at epoch 5: 0.3880665898323059\n",
      "Loss at epoch 6: 0.38805142045021057\n",
      "Loss at epoch 7: 0.3880363404750824\n",
      "Loss at epoch 8: 0.388021320104599\n",
      "Loss at epoch 9: 0.38800638914108276\n",
      "Loss at epoch 10: 0.3879915475845337\n",
      "Final Loss at epoch 10: 0.3879915475845337\n",
      "0.3879915475845337\n",
      "Round  41, Average loss 0.388, Test loss 0.435, Test accuracy: 0.77\n",
      "Loss at epoch 1: 0.3879767060279846\n",
      "Loss at epoch 2: 0.3879620432853699\n",
      "Loss at epoch 3: 0.3879474103450775\n",
      "Loss at epoch 4: 0.3879328668117523\n",
      "Loss at epoch 5: 0.3879183828830719\n",
      "Loss at epoch 6: 0.38790398836135864\n",
      "Loss at epoch 7: 0.38788968324661255\n",
      "Loss at epoch 8: 0.38787543773651123\n",
      "Loss at epoch 9: 0.3878612816333771\n",
      "Loss at epoch 10: 0.3878471851348877\n",
      "Final Loss at epoch 10: 0.3878471851348877\n",
      "0.3878471851348877\n",
      "Round  42, Average loss 0.388, Test loss 0.436, Test accuracy: 0.77\n",
      "Loss at epoch 1: 0.3878331184387207\n",
      "Loss at epoch 2: 0.38781920075416565\n",
      "Loss at epoch 3: 0.3878052830696106\n",
      "Loss at epoch 4: 0.3877914547920227\n",
      "Loss at epoch 5: 0.38777774572372437\n",
      "Loss at epoch 6: 0.387764036655426\n",
      "Loss at epoch 7: 0.38775041699409485\n",
      "Loss at epoch 8: 0.3877369165420532\n",
      "Loss at epoch 9: 0.387723445892334\n",
      "Loss at epoch 10: 0.3877100348472595\n",
      "Final Loss at epoch 10: 0.3877100348472595\n",
      "0.3877100348472595\n",
      "Round  43, Average loss 0.388, Test loss 0.437, Test accuracy: 0.77\n",
      "Loss at epoch 1: 0.3876967132091522\n",
      "Loss at epoch 2: 0.3876834511756897\n",
      "Loss at epoch 3: 0.38767027854919434\n",
      "Loss at epoch 4: 0.38765713572502136\n",
      "Loss at epoch 5: 0.38764408230781555\n",
      "Loss at epoch 6: 0.38763102889060974\n",
      "Loss at epoch 7: 0.38761812448501587\n",
      "Loss at epoch 8: 0.3876052796840668\n",
      "Loss at epoch 9: 0.38759246468544006\n",
      "Loss at epoch 10: 0.3875797390937805\n",
      "Final Loss at epoch 10: 0.3875797390937805\n",
      "0.3875797390937805\n",
      "Round  44, Average loss 0.388, Test loss 0.438, Test accuracy: 0.77\n",
      "Loss at epoch 1: 0.3875671327114105\n",
      "Loss at epoch 2: 0.387554407119751\n",
      "Loss at epoch 3: 0.3875419497489929\n",
      "Loss at epoch 4: 0.3875294625759125\n",
      "Loss at epoch 5: 0.3875170350074768\n",
      "Loss at epoch 6: 0.3875046670436859\n",
      "Loss at epoch 7: 0.38749241828918457\n",
      "Loss at epoch 8: 0.3874801695346832\n",
      "Loss at epoch 9: 0.38746798038482666\n",
      "Loss at epoch 10: 0.38745588064193726\n",
      "Final Loss at epoch 10: 0.38745588064193726\n",
      "0.38745588064193726\n",
      "Round  45, Average loss 0.387, Test loss 0.439, Test accuracy: 0.77\n",
      "Loss at epoch 1: 0.387443870306015\n",
      "Loss at epoch 2: 0.3874318599700928\n",
      "Loss at epoch 3: 0.3874199688434601\n",
      "Loss at epoch 4: 0.3874080777168274\n",
      "Loss at epoch 5: 0.3873962461948395\n",
      "Loss at epoch 6: 0.3873845040798187\n",
      "Loss at epoch 7: 0.38737285137176514\n",
      "Loss at epoch 8: 0.38736119866371155\n",
      "Loss at epoch 9: 0.3873496353626251\n",
      "Loss at epoch 10: 0.38733813166618347\n",
      "Final Loss at epoch 10: 0.38733813166618347\n",
      "0.38733813166618347\n",
      "Round  46, Average loss 0.387, Test loss 0.440, Test accuracy: 0.77\n",
      "Loss at epoch 1: 0.3873266875743866\n",
      "Loss at epoch 2: 0.38731518387794495\n",
      "Loss at epoch 3: 0.38730388879776\n",
      "Loss at epoch 4: 0.3872925937175751\n",
      "Loss at epoch 5: 0.3872813582420349\n",
      "Loss at epoch 6: 0.3872701823711395\n",
      "Loss at epoch 7: 0.38725903630256653\n",
      "Loss at epoch 8: 0.3872479498386383\n",
      "Loss at epoch 9: 0.38723695278167725\n",
      "Loss at epoch 10: 0.38722601532936096\n",
      "Final Loss at epoch 10: 0.38722601532936096\n",
      "0.38722601532936096\n",
      "Round  47, Average loss 0.387, Test loss 0.441, Test accuracy: 0.77\n",
      "Loss at epoch 1: 0.38721510767936707\n",
      "Loss at epoch 2: 0.38720425963401794\n",
      "Loss at epoch 3: 0.3871934413909912\n",
      "Loss at epoch 4: 0.38718265295028687\n",
      "Loss at epoch 5: 0.38717201352119446\n",
      "Loss at epoch 6: 0.38716134428977966\n",
      "Loss at epoch 7: 0.38715073466300964\n",
      "Loss at epoch 8: 0.3871402144432068\n",
      "Loss at epoch 9: 0.38712969422340393\n",
      "Loss at epoch 10: 0.3871192932128906\n",
      "Final Loss at epoch 10: 0.3871192932128906\n",
      "0.3871192932128906\n",
      "Round  48, Average loss 0.387, Test loss 0.441, Test accuracy: 0.77\n",
      "Loss at epoch 1: 0.3871088922023773\n",
      "Loss at epoch 2: 0.3870985805988312\n",
      "Loss at epoch 3: 0.38708823919296265\n",
      "Loss at epoch 4: 0.38707801699638367\n",
      "Loss at epoch 5: 0.38706788420677185\n",
      "Loss at epoch 6: 0.38705775141716003\n",
      "Loss at epoch 7: 0.3870476484298706\n",
      "Loss at epoch 8: 0.38703757524490356\n",
      "Loss at epoch 9: 0.3870276212692261\n",
      "Loss at epoch 10: 0.38701769709587097\n",
      "Final Loss at epoch 10: 0.38701769709587097\n",
      "0.38701769709587097\n",
      "Round  49, Average loss 0.387, Test loss 0.442, Test accuracy: 0.77\n",
      "Loss at epoch 1: 0.38700777292251587\n",
      "Loss at epoch 2: 0.38699787855148315\n",
      "Loss at epoch 3: 0.38698810338974\n",
      "Loss at epoch 4: 0.3869783282279968\n",
      "Loss at epoch 5: 0.3869686722755432\n",
      "Loss at epoch 6: 0.3869589865207672\n",
      "Loss at epoch 7: 0.3869493305683136\n",
      "Loss at epoch 8: 0.3869398534297943\n",
      "Loss at epoch 9: 0.38693031668663025\n",
      "Loss at epoch 10: 0.38692083954811096\n",
      "Final Loss at epoch 10: 0.38692083954811096\n",
      "0.38692083954811096\n",
      "Round  50, Average loss 0.387, Test loss 0.443, Test accuracy: 0.77\n",
      "Loss at epoch 1: 0.38691145181655884\n",
      "Loss at epoch 2: 0.3869020342826843\n",
      "Loss at epoch 3: 0.386892706155777\n",
      "Loss at epoch 4: 0.38688334822654724\n",
      "Loss at epoch 5: 0.38687413930892944\n",
      "Loss at epoch 6: 0.38686490058898926\n",
      "Loss at epoch 7: 0.3868557810783386\n",
      "Loss at epoch 8: 0.386846661567688\n",
      "Loss at epoch 9: 0.38683760166168213\n",
      "Loss at epoch 10: 0.3868285119533539\n",
      "Final Loss at epoch 10: 0.3868285119533539\n",
      "0.3868285119533539\n",
      "Round  51, Average loss 0.387, Test loss 0.444, Test accuracy: 0.75\n",
      "Loss at epoch 1: 0.38681960105895996\n",
      "Loss at epoch 2: 0.38681066036224365\n",
      "Loss at epoch 3: 0.38680174946784973\n",
      "Loss at epoch 4: 0.3867928683757782\n",
      "Loss at epoch 5: 0.38678407669067383\n",
      "Loss at epoch 6: 0.38677528500556946\n",
      "Loss at epoch 7: 0.38676658272743225\n",
      "Loss at epoch 8: 0.38675785064697266\n",
      "Loss at epoch 9: 0.3867492079734802\n",
      "Loss at epoch 10: 0.3867405951023102\n",
      "Final Loss at epoch 10: 0.3867405951023102\n",
      "0.3867405951023102\n",
      "Round  52, Average loss 0.387, Test loss 0.445, Test accuracy: 0.75\n",
      "Loss at epoch 1: 0.3867320418357849\n",
      "Loss at epoch 2: 0.38672348856925964\n",
      "Loss at epoch 3: 0.38671499490737915\n",
      "Loss at epoch 4: 0.38670653104782104\n",
      "Loss at epoch 5: 0.3866981267929077\n",
      "Loss at epoch 6: 0.3866897523403168\n",
      "Loss at epoch 7: 0.386681467294693\n",
      "Loss at epoch 8: 0.38667309284210205\n",
      "Loss at epoch 9: 0.3866649270057678\n",
      "Loss at epoch 10: 0.3866567015647888\n",
      "Final Loss at epoch 10: 0.3866567015647888\n",
      "0.3866567015647888\n",
      "Round  53, Average loss 0.387, Test loss 0.446, Test accuracy: 0.75\n",
      "Loss at epoch 1: 0.3866484463214874\n",
      "Loss at epoch 2: 0.386640340089798\n",
      "Loss at epoch 3: 0.3866322338581085\n",
      "Loss at epoch 4: 0.38662421703338623\n",
      "Loss at epoch 5: 0.38661617040634155\n",
      "Loss at epoch 6: 0.38660821318626404\n",
      "Loss at epoch 7: 0.3866002857685089\n",
      "Loss at epoch 8: 0.3865923285484314\n",
      "Loss at epoch 9: 0.38658446073532104\n",
      "Loss at epoch 10: 0.3865765929222107\n",
      "Final Loss at epoch 10: 0.3865765929222107\n",
      "0.3865765929222107\n",
      "Round  54, Average loss 0.387, Test loss 0.447, Test accuracy: 0.75\n",
      "Loss at epoch 1: 0.3865688443183899\n",
      "Loss at epoch 2: 0.3865610659122467\n",
      "Loss at epoch 3: 0.3865533173084259\n",
      "Loss at epoch 4: 0.38654565811157227\n",
      "Loss at epoch 5: 0.38653796911239624\n",
      "Loss at epoch 6: 0.3865303695201874\n",
      "Loss at epoch 7: 0.38652274012565613\n",
      "Loss at epoch 8: 0.3865152597427368\n",
      "Loss at epoch 9: 0.3865077495574951\n",
      "Loss at epoch 10: 0.3865002393722534\n",
      "Final Loss at epoch 10: 0.3865002393722534\n",
      "0.3865002393722534\n",
      "Round  55, Average loss 0.387, Test loss 0.447, Test accuracy: 0.75\n",
      "Loss at epoch 1: 0.3864927887916565\n",
      "Loss at epoch 2: 0.38648536801338196\n",
      "Loss at epoch 3: 0.3864780068397522\n",
      "Loss at epoch 4: 0.3864706754684448\n",
      "Loss at epoch 5: 0.38646337389945984\n",
      "Loss at epoch 6: 0.38645607233047485\n",
      "Loss at epoch 7: 0.38644886016845703\n",
      "Loss at epoch 8: 0.3864416182041168\n",
      "Loss at epoch 9: 0.3864344656467438\n",
      "Loss at epoch 10: 0.3864273130893707\n",
      "Final Loss at epoch 10: 0.3864273130893707\n",
      "0.3864273130893707\n",
      "Round  56, Average loss 0.386, Test loss 0.448, Test accuracy: 0.75\n",
      "Loss at epoch 1: 0.38642024993896484\n",
      "Loss at epoch 2: 0.3864130973815918\n",
      "Loss at epoch 3: 0.3864060938358307\n",
      "Loss at epoch 4: 0.3863990902900696\n",
      "Loss at epoch 5: 0.38639214634895325\n",
      "Loss at epoch 6: 0.38638514280319214\n",
      "Loss at epoch 7: 0.3863782584667206\n",
      "Loss at epoch 8: 0.386371374130249\n",
      "Loss at epoch 9: 0.38636454939842224\n",
      "Loss at epoch 10: 0.38635769486427307\n",
      "Final Loss at epoch 10: 0.38635769486427307\n",
      "0.38635769486427307\n",
      "Round  57, Average loss 0.386, Test loss 0.449, Test accuracy: 0.75\n",
      "Loss at epoch 1: 0.3863508999347687\n",
      "Loss at epoch 2: 0.38634416460990906\n",
      "Loss at epoch 3: 0.38633742928504944\n",
      "Loss at epoch 4: 0.3863307535648346\n",
      "Loss at epoch 5: 0.38632407784461975\n",
      "Loss at epoch 6: 0.3863174319267273\n",
      "Loss at epoch 7: 0.3863108158111572\n",
      "Loss at epoch 8: 0.38630425930023193\n",
      "Loss at epoch 9: 0.38629770278930664\n",
      "Loss at epoch 10: 0.38629117608070374\n",
      "Final Loss at epoch 10: 0.38629117608070374\n",
      "0.38629117608070374\n",
      "Round  58, Average loss 0.386, Test loss 0.450, Test accuracy: 0.75\n",
      "Loss at epoch 1: 0.386284738779068\n",
      "Loss at epoch 2: 0.38627827167510986\n",
      "Loss at epoch 3: 0.3862718343734741\n",
      "Loss at epoch 4: 0.38626542687416077\n",
      "Loss at epoch 5: 0.3862590491771698\n",
      "Loss at epoch 6: 0.3862527310848236\n",
      "Loss at epoch 7: 0.3862464129924774\n",
      "Loss at epoch 8: 0.386240154504776\n",
      "Loss at epoch 9: 0.386233925819397\n",
      "Loss at epoch 10: 0.38622766733169556\n",
      "Final Loss at epoch 10: 0.38622766733169556\n",
      "0.38622766733169556\n",
      "Round  59, Average loss 0.386, Test loss 0.451, Test accuracy: 0.75\n",
      "Loss at epoch 1: 0.38622143864631653\n",
      "Loss at epoch 2: 0.38621532917022705\n",
      "Loss at epoch 3: 0.3862091898918152\n",
      "Loss at epoch 4: 0.3862030804157257\n",
      "Loss at epoch 5: 0.38619697093963623\n",
      "Loss at epoch 6: 0.38619092106819153\n",
      "Loss at epoch 7: 0.38618484139442444\n",
      "Loss at epoch 8: 0.3861788809299469\n",
      "Loss at epoch 9: 0.38617292046546936\n",
      "Loss at epoch 10: 0.3861669600009918\n",
      "Final Loss at epoch 10: 0.3861669600009918\n",
      "0.3861669600009918\n",
      "Round  60, Average loss 0.386, Test loss 0.451, Test accuracy: 0.75\n",
      "Loss at epoch 1: 0.38616102933883667\n",
      "Loss at epoch 2: 0.3861551582813263\n",
      "Loss at epoch 3: 0.3861493170261383\n",
      "Loss at epoch 4: 0.38614344596862793\n",
      "Loss at epoch 5: 0.3861376643180847\n",
      "Loss at epoch 6: 0.3861318528652191\n",
      "Loss at epoch 7: 0.3861260414123535\n",
      "Loss at epoch 8: 0.38612034916877747\n",
      "Loss at epoch 9: 0.38611459732055664\n",
      "Loss at epoch 10: 0.38610896468162537\n",
      "Final Loss at epoch 10: 0.38610896468162537\n",
      "0.38610896468162537\n",
      "Round  61, Average loss 0.386, Test loss 0.452, Test accuracy: 0.75\n",
      "Loss at epoch 1: 0.3861033320426941\n",
      "Loss at epoch 2: 0.38609763979911804\n",
      "Loss at epoch 3: 0.38609203696250916\n",
      "Loss at epoch 4: 0.38608646392822266\n",
      "Loss at epoch 5: 0.38608092069625854\n",
      "Loss at epoch 6: 0.38607537746429443\n",
      "Loss at epoch 7: 0.3860698640346527\n",
      "Loss at epoch 8: 0.386064350605011\n",
      "Loss at epoch 9: 0.38605889678001404\n",
      "Loss at epoch 10: 0.3860534727573395\n",
      "Final Loss at epoch 10: 0.3860534727573395\n",
      "0.3860534727573395\n",
      "Round  62, Average loss 0.386, Test loss 0.453, Test accuracy: 0.75\n",
      "Loss at epoch 1: 0.3860481083393097\n",
      "Loss at epoch 2: 0.38604268431663513\n",
      "Loss at epoch 3: 0.3860373795032501\n",
      "Loss at epoch 4: 0.3860320448875427\n",
      "Loss at epoch 5: 0.38602668046951294\n",
      "Loss at epoch 6: 0.3860214054584503\n",
      "Loss at epoch 7: 0.3860161006450653\n",
      "Loss at epoch 8: 0.38601088523864746\n",
      "Loss at epoch 9: 0.386005699634552\n",
      "Loss at epoch 10: 0.38600048422813416\n",
      "Final Loss at epoch 10: 0.38600048422813416\n",
      "0.38600048422813416\n",
      "Round  63, Average loss 0.386, Test loss 0.453, Test accuracy: 0.75\n",
      "Loss at epoch 1: 0.3859952688217163\n",
      "Loss at epoch 2: 0.38599011301994324\n",
      "Loss at epoch 3: 0.38598501682281494\n",
      "Loss at epoch 4: 0.38597986102104187\n",
      "Loss at epoch 5: 0.38597482442855835\n",
      "Loss at epoch 6: 0.3859698176383972\n",
      "Loss at epoch 7: 0.3859647214412689\n",
      "Loss at epoch 8: 0.3859596848487854\n",
      "Loss at epoch 9: 0.38595470786094666\n",
      "Loss at epoch 10: 0.3859497606754303\n",
      "Final Loss at epoch 10: 0.3859497606754303\n",
      "0.3859497606754303\n",
      "Round  64, Average loss 0.386, Test loss 0.454, Test accuracy: 0.75\n",
      "Loss at epoch 1: 0.38594481348991394\n",
      "Loss at epoch 2: 0.38593989610671997\n",
      "Loss at epoch 3: 0.3859349489212036\n",
      "Loss at epoch 4: 0.38593006134033203\n",
      "Loss at epoch 5: 0.38592520356178284\n",
      "Loss at epoch 6: 0.3859204053878784\n",
      "Loss at epoch 7: 0.3859155774116516\n",
      "Loss at epoch 8: 0.3859107494354248\n",
      "Loss at epoch 9: 0.3859059810638428\n",
      "Loss at epoch 10: 0.38590121269226074\n",
      "Final Loss at epoch 10: 0.38590121269226074\n",
      "0.38590121269226074\n",
      "Round  65, Average loss 0.386, Test loss 0.455, Test accuracy: 0.75\n",
      "Loss at epoch 1: 0.3858965039253235\n",
      "Loss at epoch 2: 0.3858918249607086\n",
      "Loss at epoch 3: 0.385887086391449\n",
      "Loss at epoch 4: 0.3858824372291565\n",
      "Loss at epoch 5: 0.385877788066864\n",
      "Loss at epoch 6: 0.38587313890457153\n",
      "Loss at epoch 7: 0.38586854934692383\n",
      "Loss at epoch 8: 0.38586392998695374\n",
      "Loss at epoch 9: 0.3858593702316284\n",
      "Loss at epoch 10: 0.3858548104763031\n",
      "Final Loss at epoch 10: 0.3858548104763031\n",
      "0.3858548104763031\n",
      "Round  66, Average loss 0.386, Test loss 0.456, Test accuracy: 0.75\n",
      "Loss at epoch 1: 0.38585028052330017\n",
      "Loss at epoch 2: 0.385845810174942\n",
      "Loss at epoch 3: 0.3858413100242615\n",
      "Loss at epoch 4: 0.3858368694782257\n",
      "Loss at epoch 5: 0.38583239912986755\n",
      "Loss at epoch 6: 0.3858279287815094\n",
      "Loss at epoch 7: 0.3858235478401184\n",
      "Loss at epoch 8: 0.38581913709640503\n",
      "Loss at epoch 9: 0.3858147859573364\n",
      "Loss at epoch 10: 0.3858104348182678\n",
      "Final Loss at epoch 10: 0.3858104348182678\n",
      "0.3858104348182678\n",
      "Round  67, Average loss 0.386, Test loss 0.456, Test accuracy: 0.75\n",
      "Loss at epoch 1: 0.38580605387687683\n",
      "Loss at epoch 2: 0.385801762342453\n",
      "Loss at epoch 3: 0.3857974708080292\n",
      "Loss at epoch 4: 0.38579320907592773\n",
      "Loss at epoch 5: 0.3857889473438263\n",
      "Loss at epoch 6: 0.38578468561172485\n",
      "Loss at epoch 7: 0.3857804834842682\n",
      "Loss at epoch 8: 0.3857762813568115\n",
      "Loss at epoch 9: 0.38577210903167725\n",
      "Loss at epoch 10: 0.38576793670654297\n",
      "Final Loss at epoch 10: 0.38576793670654297\n",
      "0.38576793670654297\n",
      "Round  68, Average loss 0.386, Test loss 0.457, Test accuracy: 0.75\n",
      "Loss at epoch 1: 0.3857637643814087\n",
      "Loss at epoch 2: 0.3857596218585968\n",
      "Loss at epoch 3: 0.3857555091381073\n",
      "Loss at epoch 4: 0.3857514262199402\n",
      "Loss at epoch 5: 0.38574734330177307\n",
      "Loss at epoch 6: 0.38574329018592834\n",
      "Loss at epoch 7: 0.385739266872406\n",
      "Loss at epoch 8: 0.38573524355888367\n",
      "Loss at epoch 9: 0.38573119044303894\n",
      "Loss at epoch 10: 0.38572725653648376\n",
      "Final Loss at epoch 10: 0.38572725653648376\n",
      "0.38572725653648376\n",
      "Round  69, Average loss 0.386, Test loss 0.457, Test accuracy: 0.75\n",
      "Loss at epoch 1: 0.3857232630252838\n",
      "Loss at epoch 2: 0.385719358921051\n",
      "Loss at epoch 3: 0.38571539521217346\n",
      "Loss at epoch 4: 0.3857114613056183\n",
      "Loss at epoch 5: 0.3857074975967407\n",
      "Loss at epoch 6: 0.3857036530971527\n",
      "Loss at epoch 7: 0.3856998682022095\n",
      "Loss at epoch 8: 0.3856959342956543\n",
      "Loss at epoch 9: 0.3856920897960663\n",
      "Loss at epoch 10: 0.38568827509880066\n",
      "Final Loss at epoch 10: 0.38568827509880066\n",
      "0.38568827509880066\n",
      "Round  70, Average loss 0.386, Test loss 0.458, Test accuracy: 0.75\n",
      "Loss at epoch 1: 0.38568446040153503\n",
      "Loss at epoch 2: 0.3856806755065918\n",
      "Loss at epoch 3: 0.38567692041397095\n",
      "Loss at epoch 4: 0.3856731653213501\n",
      "Loss at epoch 5: 0.3856694996356964\n",
      "Loss at epoch 6: 0.3856657147407532\n",
      "Loss at epoch 7: 0.3856620490550995\n",
      "Loss at epoch 8: 0.385658323764801\n",
      "Loss at epoch 9: 0.38565465807914734\n",
      "Loss at epoch 10: 0.3856510519981384\n",
      "Final Loss at epoch 10: 0.3856510519981384\n",
      "0.3856510519981384\n",
      "Round  71, Average loss 0.386, Test loss 0.459, Test accuracy: 0.75\n",
      "Loss at epoch 1: 0.38564738631248474\n",
      "Loss at epoch 2: 0.38564375042915344\n",
      "Loss at epoch 3: 0.38564011454582214\n",
      "Loss at epoch 4: 0.3856365382671356\n",
      "Loss at epoch 5: 0.3856329619884491\n",
      "Loss at epoch 6: 0.3856293857097626\n",
      "Loss at epoch 7: 0.3856258988380432\n",
      "Loss at epoch 8: 0.3856223225593567\n",
      "Loss at epoch 9: 0.38561877608299255\n",
      "Loss at epoch 10: 0.3856152892112732\n",
      "Final Loss at epoch 10: 0.3856152892112732\n",
      "0.3856152892112732\n",
      "Round  72, Average loss 0.386, Test loss 0.459, Test accuracy: 0.75\n",
      "Loss at epoch 1: 0.38561180233955383\n",
      "Loss at epoch 2: 0.38560837507247925\n",
      "Loss at epoch 3: 0.3856048583984375\n",
      "Loss at epoch 4: 0.3856014609336853\n",
      "Loss at epoch 5: 0.38559800386428833\n",
      "Loss at epoch 6: 0.38559457659721375\n",
      "Loss at epoch 7: 0.38559117913246155\n",
      "Loss at epoch 8: 0.38558781147003174\n",
      "Loss at epoch 9: 0.38558444380760193\n",
      "Loss at epoch 10: 0.3855810761451721\n",
      "Final Loss at epoch 10: 0.3855810761451721\n",
      "0.3855810761451721\n",
      "Round  73, Average loss 0.386, Test loss 0.460, Test accuracy: 0.75\n",
      "Loss at epoch 1: 0.3855777382850647\n",
      "Loss at epoch 2: 0.38557443022727966\n",
      "Loss at epoch 3: 0.38557112216949463\n",
      "Loss at epoch 4: 0.385567843914032\n",
      "Loss at epoch 5: 0.38556456565856934\n",
      "Loss at epoch 6: 0.3855612874031067\n",
      "Loss at epoch 7: 0.38555800914764404\n",
      "Loss at epoch 8: 0.38555479049682617\n",
      "Loss at epoch 9: 0.3855515718460083\n",
      "Loss at epoch 10: 0.38554832339286804\n",
      "Final Loss at epoch 10: 0.38554832339286804\n",
      "0.38554832339286804\n",
      "Round  74, Average loss 0.386, Test loss 0.461, Test accuracy: 0.75\n",
      "Loss at epoch 1: 0.38554510474205017\n",
      "Loss at epoch 2: 0.3855419456958771\n",
      "Loss at epoch 3: 0.385538786649704\n",
      "Loss at epoch 4: 0.3855356276035309\n",
      "Loss at epoch 5: 0.3855324685573578\n",
      "Loss at epoch 6: 0.3855293393135071\n",
      "Loss at epoch 7: 0.38552623987197876\n",
      "Loss at epoch 8: 0.38552311062812805\n",
      "Loss at epoch 9: 0.3855200409889221\n",
      "Loss at epoch 10: 0.3855170011520386\n",
      "Final Loss at epoch 10: 0.3855170011520386\n",
      "0.3855170011520386\n",
      "Round  75, Average loss 0.386, Test loss 0.461, Test accuracy: 0.75\n",
      "Loss at epoch 1: 0.38551390171051025\n",
      "Loss at epoch 2: 0.38551080226898193\n",
      "Loss at epoch 3: 0.38550782203674316\n",
      "Loss at epoch 4: 0.38550475239753723\n",
      "Loss at epoch 5: 0.38550177216529846\n",
      "Loss at epoch 6: 0.3854987621307373\n",
      "Loss at epoch 7: 0.38549578189849854\n",
      "Loss at epoch 8: 0.3854927718639374\n",
      "Loss at epoch 9: 0.3854898512363434\n",
      "Loss at epoch 10: 0.3854869604110718\n",
      "Final Loss at epoch 10: 0.3854869604110718\n",
      "0.3854869604110718\n",
      "Round  76, Average loss 0.385, Test loss 0.462, Test accuracy: 0.75\n",
      "Loss at epoch 1: 0.3854839503765106\n",
      "Loss at epoch 2: 0.385481059551239\n",
      "Loss at epoch 3: 0.38547807931900024\n",
      "Loss at epoch 4: 0.385475218296051\n",
      "Loss at epoch 5: 0.3854723274707794\n",
      "Loss at epoch 6: 0.3854694664478302\n",
      "Loss at epoch 7: 0.385466605424881\n",
      "Loss at epoch 8: 0.38546374440193176\n",
      "Loss at epoch 9: 0.3854609429836273\n",
      "Loss at epoch 10: 0.3854580819606781\n",
      "Final Loss at epoch 10: 0.3854580819606781\n",
      "0.3854580819606781\n",
      "Round  77, Average loss 0.385, Test loss 0.462, Test accuracy: 0.75\n",
      "Loss at epoch 1: 0.38545531034469604\n",
      "Loss at epoch 2: 0.3854524493217468\n",
      "Loss at epoch 3: 0.38544970750808716\n",
      "Loss at epoch 4: 0.3854469060897827\n",
      "Loss at epoch 5: 0.38544413447380066\n",
      "Loss at epoch 6: 0.3854413628578186\n",
      "Loss at epoch 7: 0.38543862104415894\n",
      "Loss at epoch 8: 0.38543590903282166\n",
      "Loss at epoch 9: 0.3854331970214844\n",
      "Loss at epoch 10: 0.3854304552078247\n",
      "Final Loss at epoch 10: 0.3854304552078247\n",
      "0.3854304552078247\n",
      "Round  78, Average loss 0.385, Test loss 0.463, Test accuracy: 0.75\n",
      "Loss at epoch 1: 0.3854277729988098\n",
      "Loss at epoch 2: 0.3854250907897949\n",
      "Loss at epoch 3: 0.38542240858078003\n",
      "Loss at epoch 4: 0.38541972637176514\n",
      "Loss at epoch 5: 0.38541707396507263\n",
      "Loss at epoch 6: 0.3854145109653473\n",
      "Loss at epoch 7: 0.3854118287563324\n",
      "Loss at epoch 8: 0.3854092061519623\n",
      "Loss at epoch 9: 0.38540661334991455\n",
      "Loss at epoch 10: 0.38540399074554443\n",
      "Final Loss at epoch 10: 0.38540399074554443\n",
      "0.38540399074554443\n",
      "Round  79, Average loss 0.385, Test loss 0.463, Test accuracy: 0.75\n",
      "Best model, iter: 0, acc: 0.8032786846160889\n"
     ]
    }
   ],
   "source": [
    "encrypted_weights_models = {}\n",
    "encrypted_bias_models = {}\n",
    "validation_X_set = []\n",
    "validation_y_set = []\n",
    "loss_train = []\n",
    "net_best = None\n",
    "best_loss = None\n",
    "best_acc = None\n",
    "best_epoch = None\n",
    "results = []\n",
    "\n",
    "for iter in range(EPOCHS):\n",
    "    w_glob = None\n",
    "    loss_locals = []\n",
    "    for client in clients:\n",
    "        if client.name == 'Cleveland':\n",
    "            y_train, y_valid, X_train_normalized, X_valid_normalized = split_prep_data(client, pipe)\n",
    "            n_features = X_train_normalized.shape[1]\n",
    "\n",
    "            # if client.name == 'Cleveland':\n",
    "            #     validation_X_set = X_valid_normalized\n",
    "            #     validation_y_set = y_valid\n",
    "            # else:\n",
    "            #     validation_X_set = np.concatenate((validation_X_set, X_valid_normalized), axis=0)\n",
    "            #     validation_y_set = np.concatenate((validation_y_set, y_valid), axis=0)\n",
    "            net_local = copy.deepcopy(net_glob)\n",
    "            w_local, loss = train(net_local, optim, criterion, X_train_normalized, y_train)\n",
    "            \n",
    "            loss_locals.append(copy.deepcopy(loss))\n",
    "\n",
    "            if w_glob is None:\n",
    "                w_glob = copy.deepcopy(w_local)\n",
    "            else:\n",
    "                for k in w_glob.keys():\n",
    "                    w_glob[k] += w_local[k]\n",
    "\n",
    "    lr *= lr_decay\n",
    "\n",
    "    for k in w_glob.keys():\n",
    "        w_glob[k] = w_glob[k]* (1)\n",
    "\n",
    "\n",
    "    \n",
    "    net_glob.load_state_dict(w_glob)\n",
    "    loss_avg = sum(loss_locals) / len(loss_locals)\n",
    "    loss_train.append(loss_avg)\n",
    "    # if (iter + 1) % 4 == 0:\n",
    "    # print(w_glob)\n",
    "    net_glob.eval()\n",
    "    validation_y_set_tensor = y_valid\n",
    "    validation_X_set_tensor = X_valid_normalized\n",
    "    acc_test, loss_test =  accuracy_loss_LR(net_glob, validation_X_set_tensor, validation_y_set_tensor.reshape(validation_y_set_tensor.shape[:2]), criterion)\n",
    "\n",
    "    print('Round {:3d}, Average loss {:.3f}, Test loss {:.3f}, Test accuracy: {:.2f}'.format(\n",
    "        iter, loss_avg, loss_test, acc_test))\n",
    "\n",
    "\n",
    "    if best_acc is None or acc_test > best_acc:\n",
    "        net_best = copy.deepcopy(net_glob)\n",
    "        best_acc = acc_test\n",
    "        best_epoch = iter\n",
    "\n",
    "    # if (iter + 1) > args.start_saving:\n",
    "    #     model_save_path = os.path.join(base_dir, 'fed/model_{}.pt'.format(iter + 1))\n",
    "    #     torch.save(net_glob.state_dict(), model_save_path)\n",
    "\n",
    "    results.append(np.array([iter, loss_avg, loss_test, acc_test, best_acc]))\n",
    "    final_results = np.array(results)\n",
    "    final_results = pd.DataFrame(final_results, columns=['epoch', 'loss_avg', 'loss_test', 'acc_test', 'best_acc'])\n",
    "        # final_results.to_csv(results_save_path, index=False)\n",
    "\n",
    "    # if (iter + 1) % 50 == 0:\n",
    "        # best_save_path = os.path.join(base_dir, 'fed/best_{}.pt'.format(iter + 1))\n",
    "        # model_save_path = os.path.join(base_dir, 'fed/model_{}.pt'.format(iter + 1))\n",
    "        # torch.save(net_best.state_dict(), best_save_path)\n",
    "        # torch.save(net_glob.state_dict(), model_save_path)\n",
    "\n",
    "print('Best model, iter: {}, acc: {}'.format(best_epoch, best_acc))\n",
    "\n",
    "\n",
    "\n",
    "    # plain_accuracy = accuracy_LR(net_glob, validation_X_set_tensor, validation_y_set_tensor)\n",
    "    # print(f\"Accuracy on plain test_set: {plain_accuracy}\")\n",
    "    # weights = model.lr.weight.data.tolist()[0]\n",
    "    # bias = model.lr.bias.data.tolist()\n",
    "\n",
    "    # encrypted_weights = ts.ckks_vector(ctx_eval, weights)\n",
    "    # encrypted_weights_models[client.name+'_LR'] = encrypted_weights\n",
    "    # encrypted_bias = ts.ckks_vector(ctx_eval, bias)\n",
    "    # encrypted_bias_models[client.name+'_LR'] = encrypted_bias\n",
    "    # eelr = EncryptedLR(model)       \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y_valid' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/romyho/Documents/Master_Econometrie/Thesis/Python/FL_HE_2/HE_fl may.ipynb Cell 19'\u001b[0m in \u001b[0;36m<cell line: 12>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/romyho/Documents/Master_Econometrie/Thesis/Python/FL_HE_2/HE_fl%20may.ipynb#ch0000013?line=33'>34</a>\u001b[0m \u001b[39m# net_glob.load_state_dict(w_glob)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/romyho/Documents/Master_Econometrie/Thesis/Python/FL_HE_2/HE_fl%20may.ipynb#ch0000013?line=34'>35</a>\u001b[0m \u001b[39m# loss_avg = sum(loss_locals) / len(loss_locals)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/romyho/Documents/Master_Econometrie/Thesis/Python/FL_HE_2/HE_fl%20may.ipynb#ch0000013?line=35'>36</a>\u001b[0m \u001b[39m# loss_train.append(loss_avg)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/romyho/Documents/Master_Econometrie/Thesis/Python/FL_HE_2/HE_fl%20may.ipynb#ch0000013?line=36'>37</a>\u001b[0m \u001b[39m# # if (iter + 1) % 4 == 0:\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/romyho/Documents/Master_Econometrie/Thesis/Python/FL_HE_2/HE_fl%20may.ipynb#ch0000013?line=37'>38</a>\u001b[0m \u001b[39m# print(w_glob)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/romyho/Documents/Master_Econometrie/Thesis/Python/FL_HE_2/HE_fl%20may.ipynb#ch0000013?line=38'>39</a>\u001b[0m glob_model\u001b[39m.\u001b[39meval()\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/romyho/Documents/Master_Econometrie/Thesis/Python/FL_HE_2/HE_fl%20may.ipynb#ch0000013?line=39'>40</a>\u001b[0m validation_y_set_tensor \u001b[39m=\u001b[39m y_valid\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/romyho/Documents/Master_Econometrie/Thesis/Python/FL_HE_2/HE_fl%20may.ipynb#ch0000013?line=40'>41</a>\u001b[0m validation_X_set_tensor \u001b[39m=\u001b[39m X_valid_normalized\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/romyho/Documents/Master_Econometrie/Thesis/Python/FL_HE_2/HE_fl%20may.ipynb#ch0000013?line=41'>42</a>\u001b[0m acc_test, loss_test \u001b[39m=\u001b[39m  accuracy_loss_LR(glob_model, validation_X_set_tensor, validation_y_set_tensor\u001b[39m.\u001b[39mreshape(validation_y_set_tensor\u001b[39m.\u001b[39mshape[:\u001b[39m2\u001b[39m]), criterion)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'y_valid' is not defined"
     ]
    }
   ],
   "source": [
    "encrypted_weights_models = {}\n",
    "encrypted_bias_models = {}\n",
    "validation_X_set = []\n",
    "validation_y_set = []\n",
    "loss_train = []\n",
    "net_best = None\n",
    "best_loss = None\n",
    "best_acc = None\n",
    "best_epoch = None\n",
    "results = []\n",
    "\n",
    "for iter in range(EPOCHS):\n",
    "    loss_locals = []\n",
    "    client_state_dicts = []\n",
    "    for client in clients:\n",
    "\n",
    "        # if client.name == 'Cleveland':\n",
    "        #     validation_X_set = X_valid_normalized\n",
    "        #     validation_y_set = y_valid\n",
    "        # else:\n",
    "        #     validation_X_set = np.concatenate((validation_X_set, X_valid_normalized), axis=0)\n",
    "        #     validation_y_set = np.concatenate((validation_y_set, y_valid), axis=0)\n",
    "        client.set_state_dict(glob_model.state_dict())\n",
    "        client_state_dict, loss = client.train()\n",
    "        \n",
    "        loss_locals.append(copy.deepcopy(loss))\n",
    "\n",
    "        client_state_dicts.append(client_state_dict)\n",
    "\n",
    "    \n",
    "    glob_model.load_state_dict(average_state_dict(client_state_dicts))\n",
    "\n",
    "    \n",
    "    # net_glob.load_state_dict(w_glob)\n",
    "    # loss_avg = sum(loss_locals) / len(loss_locals)\n",
    "    # loss_train.append(loss_avg)\n",
    "    # # if (iter + 1) % 4 == 0:\n",
    "    # print(w_glob)\n",
    "    \n",
    "    glob_model.eval()\n",
    "    # validation_y_set_tensor = y_valid\n",
    "    # validation_X_set_tensor = X_valid_normalized\n",
    "    acc_test, loss_test =  accuracy_loss_LR(glob_model, , criterion)\n",
    "\n",
    "    print('Round {:3d}, Average loss {:.3f}, Test loss {:.3f}, Test accuracy: {:.2f}'.format(\n",
    "        iter, loss_avg, loss_test, acc_test))\n",
    "\n",
    "\n",
    "    if best_acc is None or acc_test > best_acc:\n",
    "        net_best = copy.deepcopy(glob_model)\n",
    "        best_acc = acc_test\n",
    "        best_epoch = iter\n",
    "\n",
    "    # if (iter + 1) > args.start_saving:\n",
    "    #     model_save_path = os.path.join(base_dir, 'fed/model_{}.pt'.format(iter + 1))\n",
    "    #     torch.save(net_glob.state_dict(), model_save_path)\n",
    "\n",
    "    results.append(np.array([iter, loss_avg, loss_test, acc_test, best_acc]))\n",
    "    final_results = np.array(results)\n",
    "    final_results = pd.DataFrame(final_results, columns=['epoch', 'loss_avg', 'loss_test', 'acc_test', 'best_acc'])\n",
    "        # final_results.to_csv(results_save_path, index=False)\n",
    "\n",
    "    # if (iter + 1) % 50 == 0:\n",
    "        # best_save_path = os.path.join(base_dir, 'fed/best_{}.pt'.format(iter + 1))\n",
    "        # model_save_path = os.path.join(base_dir, 'fed/model_{}.pt'.format(iter + 1))\n",
    "        # torch.save(net_best.state_dict(), best_save_path)\n",
    "        # torch.save(net_glob.state_dict(), model_save_path)\n",
    "\n",
    "print('Best model, iter: {}, acc: {}'.format(best_epoch, best_acc))\n",
    "\n",
    "\n",
    "\n",
    "    # plain_accuracy = accuracy_LR(net_glob, validation_X_set_tensor, validation_y_set_tensor)\n",
    "    # print(f\"Accuracy on plain test_set: {plain_accuracy}\")\n",
    "    # weights = model.lr.weight.data.tolist()[0]\n",
    "    # bias = model.lr.bias.data.tolist()\n",
    "\n",
    "    # encrypted_weights = ts.ckks_vector(ctx_eval, weights)\n",
    "    # encrypted_weights_models[client.name+'_LR'] = encrypted_weights\n",
    "    # encrypted_bias = ts.ckks_vector(ctx_eval, bias)\n",
    "    # encrypted_bias_models[client.name+'_LR'] = encrypted_bias\n",
    "    # eelr = EncryptedLR(model)       \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([61, 18])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_X_set.shape"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "cac72e2ca1489f7157a7b4a660893ba3b03625a75143f8c8e6188cd9e63cd5e4"
  },
  "kernelspec": {
   "display_name": "fl_test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
